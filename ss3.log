step: 1, loss: 0.0020715268328785896
step: 2, loss: 1.1920925402364446e-07
step: 3, loss: 0.0
step: 4, loss: 0.0
step: 5, loss: 0.0
step: 6, loss: 0.0
step: 7, loss: 0.0
step: 8, loss: 0.0
step: 9, loss: 0.0
step: 10, loss: 14.740243911743164
step: 11, loss: 32.6609001159668
step: 12, loss: 27.336549758911133
step: 13, loss: 20.885658264160156
step: 14, loss: 13.033675193786621
step: 15, loss: 5.088022232055664
step: 16, loss: 0.8165656328201294
step: 17, loss: 0.017965594306588173
step: 18, loss: 0.0001822131162043661
step: 19, loss: 3.5285825106257107e-07
step: 20, loss: 1.192092646817855e-08
step: 21, loss: 0.0
step: 22, loss: 23.624563217163086
step: 23, loss: 31.79952621459961
step: 24, loss: 29.7972412109375
step: 25, loss: 26.408302307128906
step: 26, loss: 22.548559188842773
step: 27, loss: 17.576824188232422
step: 28, loss: 13.578771591186523
step: 29, loss: 8.4931640625
step: 30, loss: 5.0794267654418945
step: 31, loss: 1.7796493768692017
step: 32, loss: 0.1363281011581421
step: 33, loss: 0.03304717317223549
step: 34, loss: 17.136913299560547
step: 35, loss: 18.733842849731445
step: 36, loss: 17.563875198364258
step: 37, loss: 16.33783721923828
step: 38, loss: 14.167057037353516
step: 39, loss: 11.994134902954102
step: 40, loss: 9.64018440246582
step: 41, loss: 7.3901286125183105
step: 42, loss: 5.370547294616699
step: 43, loss: 3.895294189453125
step: 44, loss: 1.5208563804626465
step: 45, loss: 0.4254707992076874
step: 46, loss: 0.0
step: 47, loss: 0.0
step: 48, loss: 0.0
step: 49, loss: 0.0
step: 50, loss: 0.0
step: 51, loss: 0.0
step: 52, loss: 0.0
step: 53, loss: 0.0
step: 54, loss: 0.0
step: 55, loss: 0.0
step: 56, loss: 0.0
step: 57, loss: 0.0
step: 58, loss: 0.0
step: 59, loss: 0.0
step: 60, loss: 0.0
step: 61, loss: 0.0
step: 62, loss: 0.0
step: 63, loss: 0.0
step: 64, loss: 0.0
step: 65, loss: 0.0
step: 66, loss: 0.0
step: 67, loss: 0.0
step: 68, loss: 0.0
step: 69, loss: 0.0
step: 70, loss: 0.0
step: 71, loss: 0.0
step: 72, loss: 0.0
step: 73, loss: 0.0
step: 74, loss: 0.0
step: 75, loss: 0.0
step: 76, loss: 0.0
step: 77, loss: 0.0
step: 78, loss: 0.0
step: 79, loss: 0.0
step: 80, loss: 0.0
step: 81, loss: 0.0
step: 82, loss: 0.0
step: 83, loss: 0.0
step: 84, loss: 0.0
step: 85, loss: 0.0
step: 86, loss: 0.0
step: 87, loss: 0.0
step: 88, loss: 0.0
step: 89, loss: 0.0
step: 90, loss: 0.0
step: 91, loss: 0.0
step: 92, loss: 0.0
step: 93, loss: 0.0
step: 94, loss: 0.0
step: 95, loss: 0.0
step: 96, loss: 0.0
step: 97, loss: 0.0
step: 98, loss: 0.0
step: 99, loss: 0.0
step: 100, loss: 0.0
step: 101, loss: 0.0
step: 102, loss: 0.0
step: 103, loss: 0.0
step: 104, loss: 0.0
step: 105, loss: 0.0
step: 106, loss: 0.0
step: 107, loss: 0.0
step: 108, loss: 0.0
step: 109, loss: 0.0
step: 110, loss: 0.0
step: 111, loss: 0.0
step: 112, loss: 0.0
step: 113, loss: 0.0
step: 114, loss: 0.0
step: 115, loss: 0.0
step: 116, loss: 0.0
step: 117, loss: 0.0
step: 118, loss: 0.0
step: 119, loss: 0.0
step: 120, loss: 0.0
step: 121, loss: 0.0
step: 122, loss: 0.0
step: 123, loss: 0.0
step: 124, loss: 0.0
step: 125, loss: 0.0
step: 126, loss: 0.0
step: 127, loss: 0.0
step: 128, loss: 0.0
step: 129, loss: 0.0
step: 130, loss: 0.0
step: 131, loss: 0.0
step: 132, loss: 0.0
step: 133, loss: 0.0
step: 134, loss: 0.0
step: 135, loss: 0.0
step: 136, loss: 0.0
step: 137, loss: 0.0
step: 138, loss: 0.0
step: 139, loss: 0.0
step: 140, loss: 0.0
step: 141, loss: 0.0
step: 142, loss: 0.0
step: 143, loss: 0.0
step: 144, loss: 0.0
step: 145, loss: 0.0
step: 146, loss: 0.0
step: 147, loss: 0.0
step: 148, loss: 0.0
step: 149, loss: 0.0
step: 150, loss: 0.0
step: 151, loss: 0.0
step: 152, loss: 0.0
step: 153, loss: 0.0
step: 154, loss: 0.0
step: 155, loss: 0.0
step: 156, loss: 0.0
step: 157, loss: 0.0
step: 158, loss: 0.0
step: 159, loss: 0.0
step: 160, loss: 0.0
step: 161, loss: 0.0
step: 162, loss: 0.0
step: 163, loss: 0.0
step: 164, loss: 0.0
step: 165, loss: 0.0
step: 166, loss: 0.0
step: 167, loss: 0.0
step: 168, loss: 0.0
step: 169, loss: 0.0
step: 170, loss: 0.0
step: 171, loss: 0.0
step: 172, loss: 0.0
step: 173, loss: 0.0
step: 174, loss: 0.0
step: 175, loss: 0.0
step: 176, loss: 0.0
step: 177, loss: 0.0
step: 178, loss: 0.0
step: 179, loss: 0.0
step: 180, loss: 0.0
step: 181, loss: 0.0
step: 182, loss: 0.0
step: 183, loss: 0.0
step: 184, loss: 0.0
step: 185, loss: 0.0
step: 186, loss: 0.0
step: 187, loss: 0.0
step: 188, loss: 0.0
step: 189, loss: 0.0
step: 190, loss: 0.0
step: 191, loss: 0.0
step: 192, loss: 0.0
step: 193, loss: 0.0
step: 194, loss: 0.0
step: 195, loss: 0.0
step: 196, loss: 0.0
step: 197, loss: 0.0
step: 198, loss: 0.0
step: 199, loss: 0.0
step: 200, loss: 0.0
step: 201, loss: 0.0
step: 202, loss: 0.0
step: 203, loss: 0.0
step: 204, loss: 0.0
step: 205, loss: 0.0
step: 206, loss: 0.0
step: 207, loss: 0.0
step: 208, loss: 0.0
step: 209, loss: 0.0
step: 210, loss: 0.0
step: 211, loss: 0.0
step: 212, loss: 0.0
step: 213, loss: 0.0
step: 214, loss: 0.0
step: 215, loss: 0.0
step: 216, loss: 0.0
step: 217, loss: 0.0
step: 218, loss: 0.0
step: 219, loss: 0.0
step: 220, loss: 0.0
step: 221, loss: 0.0
step: 222, loss: 0.0
step: 223, loss: 0.0
step: 224, loss: 0.0
step: 225, loss: 0.0
step: 226, loss: 0.2902739644050598
step: 227, loss: 0.4175962805747986
step: 228, loss: 0.9130203127861023
step: 229, loss: 0.6877623796463013
step: 230, loss: 0.5916145443916321
step: 231, loss: 0.3632395565509796
step: 232, loss: 0.3761272728443146
step: 233, loss: 0.6569324731826782
step: 234, loss: 0.44046810269355774
step: 235, loss: 0.31009817123413086
step: 236, loss: 0.4702484905719757
step: 237, loss: 0.267368882894516
step: 238, loss: 0.25389808416366577
step: 239, loss: 0.42037510871887207
step: 240, loss: 0.2667200565338135
step: 241, loss: 0.22849029302597046
step: 242, loss: 0.3254067301750183
step: 243, loss: 0.21828773617744446
step: 244, loss: 0.5043138861656189
step: 245, loss: 0.20051880180835724
step: 246, loss: 0.36331242322921753
step: 247, loss: 0.32429200410842896
step: 248, loss: 0.12615364789962769
step: 249, loss: 0.3861025273799896
step: 250, loss: 0.22535258531570435
step: 251, loss: 0.2845446765422821
step: 252, loss: 0.10039132833480835
step: 253, loss: 0.2226911336183548
step: 254, loss: 0.25477340817451477
step: 255, loss: 0.38540714979171753
step: 256, loss: 0.3684800863265991
step: 257, loss: 0.20327600836753845
step: 258, loss: 0.28008851408958435
step: 259, loss: 0.29588446021080017
step: 260, loss: 0.14872927963733673
step: 261, loss: 0.38787391781806946
step: 262, loss: 0.27151671051979065
step: 263, loss: 0.19202354550361633
step: 264, loss: 0.2947518527507782
step: 265, loss: 0.47274112701416016
step: 266, loss: 0.19914641976356506
step: 267, loss: 0.389206200838089
step: 268, loss: 0.5014531016349792
step: 269, loss: 0.17804290354251862
step: 270, loss: 0.3047237694263458
step: 271, loss: 0.22064121067523956
step: 272, loss: 0.30876660346984863
step: 273, loss: 0.31822049617767334
step: 274, loss: 0.21032314002513885
step: 275, loss: 0.22279466688632965
step: 276, loss: 0.1611657291650772
step: 277, loss: 0.16502319276332855
step: 278, loss: 0.3010554015636444
step: 279, loss: 0.390564501285553
step: 280, loss: 0.32957279682159424
step: 281, loss: 0.2662268280982971
step: 282, loss: 0.18941959738731384
step: 283, loss: 0.32655876874923706
step: 284, loss: 0.2373635470867157
step: 285, loss: 0.2292226254940033
step: 286, loss: 0.15013378858566284
step: 287, loss: 0.2403298020362854
step: 288, loss: 0.31773075461387634
step: 289, loss: 0.2961035966873169
step: 290, loss: 0.30411073565483093
step: 291, loss: 0.27998802065849304
step: 292, loss: 0.33470481634140015
step: 293, loss: 0.2496715486049652
step: 294, loss: 0.28820279240608215
step: 295, loss: 0.3954221308231354
step: 296, loss: 0.25531384348869324
step: 297, loss: 0.30174803733825684
step: 298, loss: 0.2744603753089905
step: 299, loss: 0.27517199516296387
step: 300, loss: 0.3373085856437683
step: 301, loss: 0.21619382500648499
step: 302, loss: 0.2497231662273407
step: 303, loss: 0.20200927555561066
step: 304, loss: 0.3319837152957916
step: 305, loss: 0.30287542939186096
step: 306, loss: 0.24102428555488586
step: 307, loss: 0.1894286721944809
step: 308, loss: 0.49473050236701965
step: 309, loss: 0.3463457524776459
step: 310, loss: 0.29313021898269653
step: 311, loss: 0.44341591000556946
step: 312, loss: 0.200243279337883
step: 313, loss: 0.3226192891597748
step: 314, loss: 0.21644079685211182
step: 315, loss: 0.24401482939720154
step: 316, loss: 0.21188007295131683
step: 317, loss: 0.32145747542381287
step: 318, loss: 0.30454275012016296
step: 319, loss: 0.4176229238510132
step: 320, loss: 0.21580322086811066
step: 321, loss: 0.20961421728134155
step: 322, loss: 0.2432640641927719
step: 323, loss: 0.39103591442108154
step: 324, loss: 0.24310314655303955
step: 325, loss: 0.3178132176399231
step: 326, loss: 0.1677064299583435
step: 327, loss: 0.23976293206214905
step: 328, loss: 0.3396341800689697
step: 329, loss: 0.19637919962406158
step: 330, loss: 0.2626352608203888
step: 331, loss: 0.11920635402202606
step: 332, loss: 0.1434163600206375
step: 333, loss: 0.2551494240760803
step: 334, loss: 0.06655016541481018
step: 335, loss: 0.17675252258777618
step: 336, loss: 0.3382777273654938
step: 337, loss: 0.2606978714466095
step: 338, loss: 0.1373785138130188
step: 339, loss: 0.15951552987098694
step: 340, loss: 0.20010118186473846
step: 341, loss: 0.1928616464138031
step: 342, loss: 0.3018006980419159
step: 343, loss: 0.06153401359915733
step: 344, loss: 0.2022004872560501
step: 345, loss: 0.2798163592815399
step: 346, loss: 0.09269683063030243
step: 347, loss: 0.20373669266700745
step: 348, loss: 0.18548549711704254
step: 349, loss: 0.1715838462114334
step: 350, loss: 0.2455202043056488
step: 351, loss: 0.26924189925193787
step: 352, loss: 0.16714966297149658
step: 353, loss: 0.19604259729385376
step: 354, loss: 0.22424213588237762
step: 355, loss: 0.19001036882400513
step: 356, loss: 0.35809311270713806
step: 357, loss: 0.14144472777843475
step: 358, loss: 0.23783348500728607
step: 359, loss: 0.27538615465164185
step: 360, loss: 0.16338492929935455
step: 361, loss: 0.15322591364383698
step: 362, loss: 0.18775272369384766
step: 363, loss: 0.18345308303833008
step: 364, loss: 0.1371573656797409
step: 365, loss: 0.17082969844341278
step: 366, loss: 0.2389024794101715
step: 367, loss: 0.12227112799882889
step: 368, loss: 0.19489890336990356
step: 369, loss: 0.11125034093856812
step: 370, loss: 0.19865882396697998
step: 371, loss: 0.12525400519371033
step: 372, loss: 0.18828435242176056
step: 373, loss: 0.1542637050151825
step: 374, loss: 0.22411611676216125
step: 375, loss: 0.10583244264125824
step: 376, loss: 0.12777671217918396
step: 377, loss: 0.15905378758907318
step: 378, loss: 0.1582808643579483
step: 379, loss: 0.08447935432195663
step: 380, loss: 0.2563611567020416
step: 381, loss: 0.23274385929107666
step: 382, loss: 0.10636962205171585
step: 383, loss: 0.11132091283798218
step: 384, loss: 0.2634313106536865
step: 385, loss: 0.15249645709991455
step: 386, loss: 0.28185370564460754
step: 387, loss: 0.3913264572620392
step: 388, loss: 0.18227486312389374
step: 389, loss: 0.14035995304584503
step: 390, loss: 0.1584932804107666
step: 391, loss: 0.121965691447258
step: 392, loss: 0.16158472001552582
step: 393, loss: 0.16871507465839386
step: 394, loss: 0.0621371753513813
step: 395, loss: 0.24459677934646606
step: 396, loss: 0.1636962741613388
step: 397, loss: 0.21369074285030365
step: 398, loss: 0.1181342601776123
step: 399, loss: 0.25598976016044617
step: 400, loss: 0.3006588816642761
step: 401, loss: 0.19210708141326904
step: 402, loss: 0.28403592109680176
step: 403, loss: 0.15917769074440002
step: 404, loss: 0.24966445565223694
step: 405, loss: 0.137065127491951
step: 406, loss: 0.22464412450790405
step: 407, loss: 0.0805501863360405
step: 408, loss: 0.22084426879882812
step: 409, loss: 0.11470730602741241
step: 410, loss: 0.18470709025859833
step: 411, loss: 0.12065397202968597
step: 412, loss: 0.14018894731998444
step: 413, loss: 0.09981983155012131
step: 414, loss: 0.08025039732456207
step: 415, loss: 0.13198354840278625
step: 416, loss: 0.13500405848026276
step: 417, loss: 0.12512598931789398
step: 418, loss: 0.10563749074935913
step: 419, loss: 0.06757420301437378
step: 420, loss: 0.13325287401676178
step: 421, loss: 0.1269533336162567
step: 422, loss: 0.06134583801031113
step: 423, loss: 0.07705728709697723
step: 424, loss: 0.08954010158777237
step: 425, loss: 0.07614599168300629
step: 426, loss: 0.09053037315607071
step: 427, loss: 0.05121111869812012
step: 428, loss: 0.08559700846672058
step: 429, loss: 0.05136663839221001
step: 430, loss: 0.2744279205799103
step: 431, loss: 0.15687774121761322
step: 432, loss: 0.17308185994625092
step: 433, loss: 0.20036838948726654
step: 434, loss: 0.19585998356342316
step: 435, loss: 0.1257285177707672
step: 436, loss: 0.2207120954990387
step: 437, loss: 0.13379982113838196
step: 438, loss: 0.22041983902454376
step: 439, loss: 0.3565094470977783
step: 440, loss: 0.19529423117637634
step: 441, loss: 0.22708523273468018
step: 442, loss: 0.10975433140993118
step: 443, loss: 0.18932300806045532
step: 444, loss: 0.17981275916099548
step: 445, loss: 0.2960401177406311
step: 446, loss: 0.10414684563875198
step: 447, loss: 0.18442349135875702
step: 448, loss: 0.05567451938986778
step: 449, loss: 0.1485898345708847
step: 450, loss: 0.22771292924880981
step: 451, loss: 0.1955212950706482
step: 452, loss: 0.11004799604415894
step: 453, loss: 0.17158691585063934
step: 454, loss: 0.12001962959766388
step: 455, loss: 0.09480249136686325
step: 456, loss: 0.08431638032197952
step: 457, loss: 0.11473386734724045
step: 458, loss: 0.1607949286699295
step: 459, loss: 0.09306938201189041
step: 460, loss: 0.08155428618192673
step: 461, loss: 0.10032666474580765
step: 462, loss: 0.08204397559165955
step: 463, loss: 0.1169104352593422
step: 464, loss: 0.11184494942426682
step: 465, loss: 0.05255983769893646
step: 466, loss: 0.10583294183015823
step: 467, loss: 0.12159249931573868
step: 468, loss: 0.0328010655939579
step: 469, loss: 0.09240226447582245
step: 470, loss: 0.09036553651094437
step: 471, loss: 0.17695306241512299
step: 472, loss: 0.05264697223901749
step: 473, loss: 0.11265117675065994
step: 474, loss: 0.08495455980300903
step: 475, loss: 0.04957808554172516
step: 476, loss: 0.23142753541469574
step: 477, loss: 0.04898133873939514
step: 478, loss: 0.19954116642475128
step: 479, loss: 0.08392313867807388
step: 480, loss: 0.16496166586875916
step: 481, loss: 0.06457522511482239
step: 482, loss: 0.13210737705230713
step: 483, loss: 0.09220722317695618
step: 484, loss: 0.1579733043909073
step: 485, loss: 0.04862060397863388
step: 486, loss: 0.07029291242361069
step: 487, loss: 0.07517348229885101
step: 488, loss: 0.05090953782200813
step: 489, loss: 0.042617153376340866
step: 490, loss: 0.05566657707095146
step: 491, loss: 0.03177482634782791
step: 492, loss: 0.07388448715209961
step: 493, loss: 0.03659043461084366
step: 494, loss: 0.10900242626667023
step: 495, loss: 0.04371906816959381
step: 496, loss: 0.009877718985080719
step: 497, loss: 0.04492631554603577
step: 498, loss: 0.015353846363723278
step: 499, loss: 0.11800084263086319
step: 500, loss: 0.10213055461645126
step: 501, loss: 0.03433013707399368
step: 502, loss: 0.029849786311388016
step: 503, loss: 0.08837524056434631
step: 504, loss: 0.12900662422180176
step: 505, loss: 0.2968847155570984
step: 506, loss: 0.21604883670806885
step: 507, loss: 0.05530770868062973
step: 508, loss: 0.14895124733448029
step: 509, loss: 0.08838281780481339
step: 510, loss: 0.09832821786403656
step: 511, loss: 0.0767328068614006
step: 512, loss: 0.04093378037214279
step: 513, loss: 0.12523268163204193
step: 514, loss: 0.21486511826515198
step: 515, loss: 0.1615247130393982
step: 516, loss: 0.21952125430107117
step: 517, loss: 0.0904904380440712
step: 518, loss: 0.06831829994916916
step: 519, loss: 0.04309563711285591
step: 520, loss: 0.10079517215490341
step: 521, loss: 0.09441433846950531
step: 522, loss: 0.13758422434329987
step: 523, loss: 0.16833679378032684
step: 524, loss: 0.11046480387449265
step: 525, loss: 0.08144588768482208
step: 526, loss: 0.1554119735956192
step: 527, loss: 0.08376988768577576
step: 528, loss: 0.12160804867744446
step: 529, loss: 0.1045609638094902
step: 530, loss: 0.1144312173128128
step: 531, loss: 0.06625721603631973
step: 532, loss: 0.15155147016048431
step: 533, loss: 0.3123253881931305
step: 534, loss: 0.13811591267585754
step: 535, loss: 0.13300469517707825
step: 536, loss: 0.1268845647573471
step: 537, loss: 0.16724571585655212
step: 538, loss: 0.09827920794487
step: 539, loss: 0.11478985846042633
step: 540, loss: 0.12809805572032928
step: 541, loss: 0.1694740504026413
step: 542, loss: 0.12674275040626526
step: 543, loss: 0.08634237200021744
step: 544, loss: 0.15365423262119293
step: 545, loss: 0.20627860724925995
step: 546, loss: 0.1332472413778305
step: 547, loss: 0.13070812821388245
step: 548, loss: 0.21474750339984894
step: 549, loss: 0.07732653617858887
step: 550, loss: 0.10159310698509216
step: 551, loss: 0.08487695455551147
step: 552, loss: 0.15845774114131927
step: 553, loss: 0.1440429538488388
step: 554, loss: 0.15530359745025635
step: 555, loss: 0.07132043689489365
step: 556, loss: 0.12082330882549286
step: 557, loss: 0.151006817817688
step: 558, loss: 0.11143504083156586
step: 559, loss: 0.14955514669418335
step: 560, loss: 0.0319393016397953
step: 561, loss: 0.08763736486434937
step: 562, loss: 0.0773535668849945
step: 563, loss: 0.09930089116096497
step: 564, loss: 0.08995411545038223
step: 565, loss: 0.09349101781845093
step: 566, loss: 0.06499648839235306
step: 567, loss: 0.1584845632314682
step: 568, loss: 0.10253746807575226
step: 569, loss: 0.12172874808311462
step: 570, loss: 0.10083961486816406
step: 571, loss: 0.11243773251771927
step: 572, loss: 0.1612880527973175
step: 573, loss: 0.056249771267175674
step: 574, loss: 0.25771138072013855
step: 575, loss: 0.05844497308135033
step: 576, loss: 0.240705206990242
step: 577, loss: 0.11723291128873825
step: 578, loss: 0.11985069513320923
step: 579, loss: 0.06932729482650757
step: 580, loss: 0.10539861768484116
step: 581, loss: 0.2143750935792923
step: 582, loss: 0.1792965531349182
step: 583, loss: 0.06178908795118332
step: 584, loss: 0.2504864037036896
step: 585, loss: 0.11685159802436829
step: 586, loss: 0.012280524708330631
step: 587, loss: 0.09679334610700607
step: 588, loss: 0.12856966257095337
step: 589, loss: 0.170634463429451
step: 590, loss: 0.10340671241283417
step: 591, loss: 0.056464262306690216
step: 592, loss: 0.07803422957658768
step: 593, loss: 0.024535732343792915
step: 594, loss: 0.08953557908535004
step: 595, loss: 0.09131599217653275
step: 596, loss: 0.21020792424678802
step: 597, loss: 0.0932716354727745
step: 598, loss: 0.08152320235967636
step: 599, loss: 0.226742222905159
step: 600, loss: 0.08985448628664017
step: 601, loss: 0.09570138901472092
step: 602, loss: 0.07875064015388489
step: 603, loss: 0.12721696496009827
step: 604, loss: 0.054275721311569214
step: 605, loss: 0.0745527520775795
step: 606, loss: 0.1947193294763565
step: 607, loss: 0.08523431420326233
step: 608, loss: 0.09625860303640366
step: 609, loss: 0.29132115840911865
step: 610, loss: 0.2345244288444519
step: 611, loss: 0.3278649151325226
step: 612, loss: 0.19923759996891022
step: 613, loss: 0.2861960530281067
step: 614, loss: 0.21422424912452698
step: 615, loss: 0.21570205688476562
step: 616, loss: 0.15509705245494843
step: 617, loss: 0.05804121494293213
step: 618, loss: 0.111881323158741
step: 619, loss: 0.16798463463783264
step: 620, loss: 0.13140787184238434
step: 621, loss: 0.1774769276380539
step: 622, loss: 0.13336463272571564
step: 623, loss: 0.11711227148771286
step: 624, loss: 0.285955548286438
step: 625, loss: 0.18181400001049042
step: 626, loss: 0.19374233484268188
step: 627, loss: 0.25573521852493286
step: 628, loss: 0.11958679556846619
step: 629, loss: 0.20607925951480865
step: 630, loss: 0.2046484351158142
step: 631, loss: 0.2202344834804535
step: 632, loss: 0.1604732722043991
step: 633, loss: 0.09906511008739471
step: 634, loss: 0.11813230812549591
step: 635, loss: 0.14721986651420593
step: 636, loss: 0.08529482036828995
step: 637, loss: 0.249689981341362
step: 638, loss: 0.12483641505241394
step: 639, loss: 0.1242971420288086
step: 640, loss: 0.141460120677948
step: 641, loss: 0.12093532830476761
step: 642, loss: 0.05480801686644554
step: 643, loss: 0.09484634548425674
step: 644, loss: 0.13532033562660217
step: 645, loss: 0.12204516679048538
step: 646, loss: 0.22019222378730774
step: 647, loss: 0.13079287111759186
step: 648, loss: 0.15336138010025024
step: 649, loss: 0.1898522526025772
step: 650, loss: 0.10570462048053741
step: 651, loss: 0.09976805001497269
step: 652, loss: 0.21376237273216248
step: 653, loss: 0.09322725236415863
step: 654, loss: 0.22624893486499786
step: 655, loss: 0.1823166012763977
step: 656, loss: 0.19498105347156525
step: 657, loss: 0.21777339279651642
step: 658, loss: 0.13895860314369202
step: 659, loss: 0.11213425546884537
step: 660, loss: 0.14091405272483826
step: 661, loss: 0.12317592650651932
step: 662, loss: 0.14605623483657837
step: 663, loss: 0.19024427235126495
step: 664, loss: 0.1197463870048523
step: 665, loss: 0.06347418576478958
step: 666, loss: 0.10614518821239471
step: 667, loss: 0.19366875290870667
step: 668, loss: 0.10605181753635406
step: 669, loss: 0.06360404938459396
step: 670, loss: 0.0678010880947113
step: 671, loss: 0.15076009929180145
step: 672, loss: 0.06572101265192032
step: 673, loss: 0.15779125690460205
step: 674, loss: 0.17001892626285553
step: 675, loss: 0.05288584530353546
step: 676, loss: 0.08959297090768814
step: 677, loss: 0.05319507420063019
step: 678, loss: 0.09549620747566223
step: 679, loss: 0.18375274538993835
step: 680, loss: 0.07917819172143936
step: 681, loss: 0.0915648490190506
step: 682, loss: 0.15030823647975922
step: 683, loss: 0.22311288118362427
step: 684, loss: 0.1444132775068283
step: 685, loss: 0.09465839713811874
step: 686, loss: 0.12269517779350281
step: 687, loss: 0.24777521193027496
step: 688, loss: 0.15271136164665222
step: 689, loss: 0.15506067872047424
step: 690, loss: 0.1498686522245407
step: 691, loss: 0.01402316801249981
step: 692, loss: 0.06165417283773422
step: 693, loss: 0.1809631586074829
step: 694, loss: 0.05156194791197777
step: 695, loss: 0.0698343813419342
step: 696, loss: 0.06505345553159714
step: 697, loss: 0.18892575800418854
step: 698, loss: 0.06522025913000107
step: 699, loss: 0.061017587780952454
step: 700, loss: 0.19365310668945312
step: 701, loss: 0.16811686754226685
step: 702, loss: 0.08556361496448517
step: 703, loss: 0.08318527042865753
step: 704, loss: 0.11988023668527603
step: 705, loss: 0.05746433138847351
step: 706, loss: 0.059606149792671204
step: 707, loss: 0.055385101586580276
step: 708, loss: 0.11500176787376404
step: 709, loss: 0.06749057024717331
step: 710, loss: 0.1251116544008255
step: 711, loss: 0.02733459696173668
step: 712, loss: 0.11012856662273407
step: 713, loss: 0.057484112679958344
step: 714, loss: 0.17328324913978577
step: 715, loss: 0.11924519389867783
step: 716, loss: 0.04881325736641884
step: 717, loss: 0.005549533758312464
step: 718, loss: 0.21347343921661377
step: 719, loss: 0.07683060318231583
step: 720, loss: 0.0464899055659771
step: 721, loss: 0.0701008290052414
step: 722, loss: 0.14062385261058807
step: 723, loss: 0.07669110596179962
step: 724, loss: 0.11760177463293076
step: 725, loss: 0.08613769710063934
step: 726, loss: 0.0851803719997406
step: 727, loss: 0.084449402987957
step: 728, loss: 0.07519963383674622
step: 729, loss: 0.02909216098487377
step: 730, loss: 0.041142016649246216
step: 731, loss: 0.10959415137767792
step: 732, loss: 0.04894562065601349
step: 733, loss: 0.17545227706432343
step: 734, loss: 0.020376672968268394
step: 735, loss: 0.020388303324580193
step: 736, loss: 0.07453231513500214
step: 737, loss: 0.06406524777412415
step: 738, loss: 0.11810434609651566
step: 739, loss: 0.023144418373703957
step: 740, loss: 0.08264770358800888
step: 741, loss: 0.18682309985160828
step: 742, loss: 0.15728233754634857
step: 743, loss: 0.008585109375417233
step: 744, loss: 0.049230195581912994
step: 745, loss: 0.1523820459842682
step: 746, loss: 0.07252509146928787
step: 747, loss: 0.15414901077747345
step: 748, loss: 0.08355880528688431
step: 749, loss: 0.0378292053937912
step: 750, loss: 0.010464780032634735
step: 751, loss: 0.10247909277677536
step: 752, loss: 0.10801123827695847
step: 753, loss: 0.15752340853214264
step: 754, loss: 0.15785256028175354
step: 755, loss: 0.0626363530755043
step: 756, loss: 0.08106013387441635
step: 757, loss: 0.14106710255146027
step: 758, loss: 0.14140072464942932
step: 759, loss: 0.09972503036260605
step: 760, loss: 0.2544195055961609
step: 761, loss: 0.14392684400081635
step: 762, loss: 0.0789356455206871
step: 763, loss: 0.06485404074192047
step: 764, loss: 0.15109114348888397
step: 765, loss: 0.11824408173561096
step: 766, loss: 0.07259824126958847
step: 767, loss: 0.06574821472167969
step: 768, loss: 0.044856104999780655
step: 769, loss: 0.12052211165428162
step: 770, loss: 0.11480528116226196
step: 771, loss: 0.10950019210577011
step: 772, loss: 0.12369802594184875
step: 773, loss: 0.13746872544288635
step: 774, loss: 0.03376403823494911
step: 775, loss: 0.2603284418582916
step: 776, loss: 0.08865661919116974
step: 777, loss: 0.19821737706661224
step: 778, loss: 0.12015975266695023
step: 779, loss: 0.35911956429481506
step: 780, loss: 0.1809251755475998
step: 781, loss: 0.1488124281167984
step: 782, loss: 0.19403405487537384
step: 783, loss: 0.19400566816329956
step: 784, loss: 0.08012576401233673
step: 785, loss: 0.037124842405319214
step: 786, loss: 0.0657605454325676
step: 787, loss: 0.0981823280453682
step: 788, loss: 0.027650771662592888
step: 789, loss: 0.08364477008581161
step: 790, loss: 0.1048298254609108
step: 791, loss: 0.05756430700421333
step: 792, loss: 0.10175520926713943
step: 793, loss: 0.1440429836511612
step: 794, loss: 0.032996710389852524
step: 795, loss: 0.12021732330322266
step: 796, loss: 0.15897971391677856
step: 797, loss: 0.08197443187236786
step: 798, loss: 0.16538216173648834
step: 799, loss: 0.027417592704296112
step: 800, loss: 0.15264125168323517
step: 801, loss: 0.07775858044624329
step: 802, loss: 0.09472527354955673
step: 803, loss: 0.13281971216201782
step: 804, loss: 0.13691826164722443
step: 805, loss: 0.128506138920784
step: 806, loss: 0.09249255806207657
step: 807, loss: 0.11018072813749313
step: 808, loss: 0.1267157346010208
step: 809, loss: 0.11640007048845291
step: 810, loss: 0.08682731539011002
step: 811, loss: 0.14439690113067627
step: 812, loss: 0.09061851352453232
step: 813, loss: 0.08054160326719284
step: 814, loss: 0.12598133087158203
step: 815, loss: 0.05855643376708031
step: 816, loss: 0.05731966346502304
step: 817, loss: 0.1211555004119873
step: 818, loss: 0.17798840999603271
step: 819, loss: 0.12594294548034668
step: 820, loss: 0.15720093250274658
step: 821, loss: 0.128725066781044
step: 822, loss: 0.09567782282829285
step: 823, loss: 0.06388859450817108
step: 824, loss: 0.1331874430179596
step: 825, loss: 0.053306687623262405
step: 826, loss: 0.038279056549072266
step: 827, loss: 0.0769021064043045
step: 828, loss: 0.0862758532166481
step: 829, loss: 0.18481016159057617
step: 830, loss: 0.11997247487306595
step: 831, loss: 0.15011169016361237
step: 832, loss: 0.17860250174999237
step: 833, loss: 0.04740045592188835
step: 834, loss: 0.14879682660102844
step: 835, loss: 0.08804245293140411
step: 836, loss: 0.15351524949073792
step: 837, loss: 0.1520683467388153
step: 838, loss: 0.13586662709712982
step: 839, loss: 0.10999612510204315
step: 840, loss: 0.10358083993196487
step: 841, loss: 0.13592970371246338
step: 842, loss: 0.20311953127384186
step: 843, loss: 0.08599915355443954
step: 844, loss: 0.17855173349380493
step: 845, loss: 0.17454442381858826
step: 846, loss: 0.15648315846920013
step: 847, loss: 0.195109561085701
step: 848, loss: 0.10645096004009247
step: 849, loss: 0.09413749724626541
step: 850, loss: 0.05452706292271614
step: 851, loss: 0.15056534111499786
step: 852, loss: 0.03803335875272751
step: 853, loss: 0.13396064937114716
step: 854, loss: 0.08177725970745087
step: 855, loss: 0.08509759604930878
step: 856, loss: 0.1440960317850113
step: 857, loss: 0.14502830803394318
step: 858, loss: 0.1965225338935852
step: 859, loss: 0.13014023005962372
step: 860, loss: 0.1117120012640953
step: 861, loss: 0.026797164231538773
step: 862, loss: 0.1207769364118576
step: 863, loss: 0.14208637177944183
step: 864, loss: 0.007805626839399338
step: 865, loss: 0.07339967042207718
step: 866, loss: 0.2057218998670578
step: 867, loss: 0.14246132969856262
step: 868, loss: 0.14419716596603394
step: 869, loss: 0.047696709632873535
step: 870, loss: 0.133442685008049
step: 871, loss: 0.10863969475030899
step: 872, loss: 0.17908893525600433
step: 873, loss: 0.09145437180995941
step: 874, loss: 0.08070514351129532
step: 875, loss: 0.09005362540483475
step: 876, loss: 0.10829925537109375
step: 877, loss: 0.06456032395362854
step: 878, loss: 0.05806231498718262
step: 879, loss: 0.10033872723579407
step: 880, loss: 0.07157895714044571
step: 881, loss: 0.0649145096540451
step: 882, loss: 0.09573953598737717
step: 883, loss: 0.09703448414802551
step: 884, loss: 0.2236567735671997
step: 885, loss: 0.18474945425987244
step: 886, loss: 0.0554373562335968
step: 887, loss: 0.10132652521133423
step: 888, loss: 0.06402263790369034
step: 889, loss: 0.09019786864519119
step: 890, loss: 0.15389512479305267
step: 891, loss: 0.09828035533428192
step: 892, loss: 0.11102192848920822
step: 893, loss: 0.14291298389434814
step: 894, loss: 0.10218082368373871
step: 895, loss: 0.148470938205719
step: 896, loss: 0.10337495058774948
step: 897, loss: 0.11087431013584137
step: 898, loss: 0.0014808392152190208
step: 899, loss: 0.11980486661195755
step: 900, loss: 0.07031545042991638
step: 901, loss: 0.059299446642398834
step: 902, loss: 0.031050996854901314
step: 903, loss: 0.030090009793639183
step: 904, loss: 0.055510394275188446
step: 905, loss: 0.08620016276836395
step: 906, loss: 0.13343504071235657
step: 907, loss: 0.13671241700649261
step: 908, loss: 0.17037178575992584
step: 909, loss: 0.048480890691280365
step: 910, loss: 0.0888662040233612
step: 911, loss: 0.05639161914587021
step: 912, loss: 0.08646682649850845
step: 913, loss: 0.10280453413724899
step: 914, loss: 0.15328402817249298
step: 915, loss: 0.087890625
step: 916, loss: 0.023739004507660866
step: 917, loss: 0.10413698107004166
step: 918, loss: 0.09272491186857224
step: 919, loss: 0.11951987445354462
step: 920, loss: 0.12343233078718185
step: 921, loss: 0.05475981533527374
step: 922, loss: 0.12361976504325867
step: 923, loss: 0.07081427425146103
step: 924, loss: 0.2464689016342163
step: 925, loss: 0.07478418201208115
step: 926, loss: 0.0883810967206955
step: 927, loss: 0.10427460074424744
step: 928, loss: 0.0944460779428482
step: 929, loss: 0.11493212729692459
step: 930, loss: 0.09343679249286652
step: 931, loss: 0.12653009593486786
step: 932, loss: 0.09151270985603333
step: 933, loss: 0.07904127240180969
step: 934, loss: 0.047770000994205475
step: 935, loss: 0.161343514919281
step: 936, loss: 0.14970393478870392
step: 937, loss: 0.05775245651602745
step: 938, loss: 0.12450960278511047
step: 939, loss: 0.03733030706644058
step: 940, loss: 0.08296984434127808
step: 941, loss: 0.06727942079305649
step: 942, loss: 0.07217825204133987
step: 943, loss: 0.08893004059791565
step: 944, loss: 0.11268708854913712
step: 945, loss: 0.13099336624145508
step: 946, loss: 0.07420019805431366
step: 947, loss: 0.052639007568359375
step: 948, loss: 0.052334968000650406
step: 949, loss: 0.09985920786857605
step: 950, loss: 0.06475226581096649
step: 951, loss: 0.00744172278791666
step: 952, loss: 0.267559289932251
step: 953, loss: 0.07996876537799835
step: 954, loss: 0.04945070669054985
step: 955, loss: 0.0820600688457489
step: 956, loss: 0.17678231000900269
step: 957, loss: 0.04610851779580116
step: 958, loss: 0.0452822744846344
step: 959, loss: 0.09228908270597458
step: 960, loss: 0.1559026539325714
step: 961, loss: 0.09015209227800369
step: 962, loss: 0.058686889708042145
step: 963, loss: 0.14731767773628235
step: 964, loss: 0.0523817241191864
step: 965, loss: 0.14649054408073425
step: 966, loss: 0.15600425004959106
step: 967, loss: 0.20094960927963257
step: 968, loss: 0.04510068893432617
step: 969, loss: 0.06609317660331726
step: 970, loss: 0.0940946415066719
step: 971, loss: 0.008684564381837845
step: 972, loss: 0.13083592057228088
step: 973, loss: 0.05972762033343315
step: 974, loss: 0.1093287244439125
step: 975, loss: 0.04592609032988548
step: 976, loss: 0.07012611627578735
step: 977, loss: 0.14132088422775269
step: 978, loss: 0.028362322598695755
step: 979, loss: 0.013235338032245636
step: 980, loss: 0.13432975113391876
step: 981, loss: 0.053167618811130524
step: 982, loss: 0.10671155154705048
step: 983, loss: 0.043428339064121246
step: 984, loss: 0.04656469449400902
step: 985, loss: 0.1322859674692154
step: 986, loss: 0.19563257694244385
step: 987, loss: 0.0716572031378746
step: 988, loss: 0.08433840423822403
step: 989, loss: 0.19571813941001892
step: 990, loss: 0.03216897323727608
step: 991, loss: 0.05152016133069992
step: 992, loss: 0.05545820668339729
step: 993, loss: 0.06495437771081924
step: 994, loss: 0.06639346480369568
step: 995, loss: 0.17816106975078583
step: 996, loss: 0.18138530850410461
step: 997, loss: 0.022113166749477386
step: 998, loss: 0.1180165708065033
step: 999, loss: 0.08716778457164764
step: 1000, loss: 0.035765811800956726
step: 1001, loss: 0.10279840230941772
step: 1002, loss: 0.08651719987392426
step: 1003, loss: 0.09641017019748688
step: 1004, loss: 0.04041554406285286
step: 1005, loss: 0.06474775075912476
step: 1006, loss: 0.03085847571492195
step: 1007, loss: 0.10084401816129684
step: 1008, loss: 0.1447872668504715
step: 1009, loss: 0.02055453695356846
step: 1010, loss: 0.06775187700986862
step: 1011, loss: 0.12822328507900238
step: 1012, loss: 0.13631229102611542
step: 1013, loss: 0.18911747634410858
step: 1014, loss: 0.02981010638177395
step: 1015, loss: 0.12908031046390533
step: 1016, loss: 0.07884450256824493
step: 1017, loss: 0.023580558598041534
step: 1018, loss: 0.023012414574623108
step: 1019, loss: 0.0008028948795981705
step: 1020, loss: 0.06205520033836365
step: 1021, loss: 0.13679569959640503
step: 1022, loss: 0.03656892850995064
step: 1023, loss: 0.05426795780658722
step: 1024, loss: 0.05624201148748398
step: 1025, loss: 0.10253623127937317
step: 1026, loss: 0.02444366365671158
step: 1027, loss: 0.058171436190605164
step: 1028, loss: 0.12338588386774063
step: 1029, loss: 0.038462113589048386
step: 1030, loss: 0.16995729506015778
step: 1031, loss: 0.0015984384808689356
step: 1032, loss: 0.0826343521475792
step: 1033, loss: 0.05003691837191582
step: 1034, loss: 0.10695801675319672
step: 1035, loss: 0.05157828703522682
step: 1036, loss: 0.142765611410141
step: 1037, loss: 0.12036798149347305
step: 1038, loss: 0.08498408645391464
step: 1039, loss: 0.08485835045576096
step: 1040, loss: 0.03960605338215828
step: 1041, loss: 0.10570516437292099
step: 1042, loss: 0.11205272376537323
step: 1043, loss: 0.07818412780761719
step: 1044, loss: 0.11296042799949646
step: 1045, loss: 0.11373097449541092
step: 1046, loss: 0.17753402888774872
step: 1047, loss: 0.07257156819105148
step: 1048, loss: 0.15735504031181335
step: 1049, loss: 0.12183615565299988
step: 1050, loss: 0.11631567776203156
step: 1051, loss: 0.08420614153146744
step: 1052, loss: 0.11568018049001694
step: 1053, loss: 0.10571009665727615
step: 1054, loss: 0.1758926957845688
step: 1055, loss: 0.07491859048604965
step: 1056, loss: 0.1703498214483261
step: 1057, loss: 0.14646829664707184
step: 1058, loss: 0.15096813440322876
step: 1059, loss: 0.1740705519914627
step: 1060, loss: 0.13077384233474731
step: 1061, loss: 0.22517341375350952
step: 1062, loss: 0.1859799176454544
step: 1063, loss: 0.08365129679441452
step: 1064, loss: 0.09516548365354538
step: 1065, loss: 0.1345946341753006
step: 1066, loss: 0.10256502032279968
step: 1067, loss: 0.21555784344673157
step: 1068, loss: 0.14437705278396606
step: 1069, loss: 0.30827969312667847
step: 1070, loss: 0.053012870252132416
step: 1071, loss: 0.17979782819747925
step: 1072, loss: 0.08834264427423477
step: 1073, loss: 0.10653901100158691
step: 1074, loss: 0.08887947350740433
step: 1075, loss: 0.11811422556638718
step: 1076, loss: 0.09300130605697632
step: 1077, loss: 0.195245623588562
step: 1078, loss: 0.18588753044605255
step: 1079, loss: 0.3085382580757141
step: 1080, loss: 0.09335246682167053
step: 1081, loss: 0.1073906347155571
step: 1082, loss: 0.24723099172115326
step: 1083, loss: 0.21022063493728638
step: 1084, loss: 0.22031734883785248
step: 1085, loss: 0.14503075182437897
step: 1086, loss: 0.09561285376548767
step: 1087, loss: 0.167706698179245
step: 1088, loss: 0.16784724593162537
step: 1089, loss: 0.18753433227539062
step: 1090, loss: 0.16132736206054688
step: 1091, loss: 0.13775689899921417
step: 1092, loss: 0.11610643565654755
step: 1093, loss: 0.24317678809165955
step: 1094, loss: 0.19012083113193512
step: 1095, loss: 0.17085331678390503
step: 1096, loss: 0.10695594549179077
step: 1097, loss: 0.18922078609466553
step: 1098, loss: 0.14901813864707947
step: 1099, loss: 0.12552356719970703
step: 1100, loss: 0.1396007388830185
step: 1101, loss: 0.09296932071447372
step: 1102, loss: 0.1305135041475296
step: 1103, loss: 0.0494728609919548
step: 1104, loss: 0.11329051852226257
step: 1105, loss: 0.08549326658248901
step: 1106, loss: 0.17953266203403473
step: 1107, loss: 0.039363183081150055
step: 1108, loss: 0.0758807510137558
step: 1109, loss: 0.1972772777080536
step: 1110, loss: 0.22659307718276978
step: 1111, loss: 0.11726492643356323
step: 1112, loss: 0.13022735714912415
step: 1113, loss: 0.07726511359214783
step: 1114, loss: 0.11899692565202713
step: 1115, loss: 0.10491415113210678
step: 1116, loss: 0.17745746672153473
step: 1117, loss: 0.1931789070367813
step: 1118, loss: 0.15755821764469147
step: 1119, loss: 0.1608666628599167
step: 1120, loss: 0.12958817183971405
step: 1121, loss: 0.07849743962287903
step: 1122, loss: 0.12946395576000214
step: 1123, loss: 0.12302235513925552
step: 1124, loss: 0.15559130907058716
step: 1125, loss: 0.05762272700667381
step: 1126, loss: 0.13687177002429962
step: 1127, loss: 0.09514632821083069
step: 1128, loss: 0.149440735578537
step: 1129, loss: 0.0917137935757637
step: 1130, loss: 0.14157547056674957
step: 1131, loss: 0.0888994112610817
step: 1132, loss: 0.09832830727100372
step: 1133, loss: 0.11436039209365845
step: 1134, loss: 0.07077112793922424
step: 1135, loss: 0.09008128196001053
step: 1136, loss: 0.16613954305648804
step: 1137, loss: 0.11786551773548126
step: 1138, loss: 0.12939593195915222
step: 1139, loss: 0.06059230491518974
step: 1140, loss: 0.19478069245815277
step: 1141, loss: 0.136543869972229
step: 1142, loss: 0.14699068665504456
step: 1143, loss: 0.08285403996706009
step: 1144, loss: 0.14193816483020782
step: 1145, loss: 0.09764283895492554
step: 1146, loss: 0.11376695334911346
step: 1147, loss: 0.01701190136373043
step: 1148, loss: 0.02372409775853157
step: 1149, loss: 0.09571515768766403
step: 1150, loss: 0.09551028907299042
step: 1151, loss: 0.19910620152950287
step: 1152, loss: 0.12068870663642883
step: 1153, loss: 0.1548129916191101
step: 1154, loss: 0.10687343776226044
step: 1155, loss: 0.2735390365123749
step: 1156, loss: 0.0792914554476738
step: 1157, loss: 0.07183907181024551
step: 1158, loss: 0.1426980346441269
step: 1159, loss: 0.09891488403081894
step: 1160, loss: 0.07009974122047424
step: 1161, loss: 0.08556485921144485
step: 1162, loss: 0.17235222458839417
step: 1163, loss: 0.08375081419944763